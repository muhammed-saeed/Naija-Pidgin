


























































































































































































































































































































































































































































































































































































































Epochs 1/2. Running Loss:    2.0783:  20%|â–ˆâ–ˆâ–Š           | 2499/12489 [20:06<1:/local/home/CE/musaeed/transformers/src/transformers/tokenization_utils_base.py:3671: FutureWarning:
`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular
`__call__` method to prepare your inputs and targets.| 0/4206 [00:00<?, ?it/s]
Here is a short example:
model_inputs = tokenizer(src_texts, text_target=tgt_texts, ...)
If you either need to use different keyword arguments for the source and target texts, you should do two calls like
this:
model_inputs = tokenizer(src_texts, ...)
labels = tokenizer(text_target=tgt_texts, ...)
model_inputs["labels"] = labels["input_ids"]
See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.
For a more complete example, see the implementation of `prepare_seq2seq_batch`.
  warnings.warn(formatted_warning, FutureWarning)
/local/home/CE/musaeed/transformers/src/transformers/tokenization_utils_base.py:3545: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/local/home/CE/musaeed/transformers/src/transformers/tokenization_utils_base.py:3671: FutureWarning:
`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular
`__call__` method to prepare your inputs and targets.
Here is a short example:
model_inputs = tokenizer(src_texts, text_target=tgt_texts, ...)
If you either need to use different keyword arguments for the source and target texts, you should do two calls like
this:
model_inputs = tokenizer(src_texts, ...)
labels = tokenizer(text_target=tgt_texts, ...)
model_inputs["labels"] = labels["input_ids"]
See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.
For a more complete example, see the implementation of `prepare_seq2seq_batch`.
  warnings.warn(formatted_warning, FutureWarning)
/local/home/CE/musaeed/transformers/src/transformers/tokenization_utils_base.py:3545: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/local/home/CE/musaeed/transformers/src/transformers/tokenization_utils_base.py:3671: FutureWarning:
`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular
`__call__` method to prepare your inputs and targets.
Here is a short example:
model_inputs = tokenizer(src_texts, text_target=tgt_texts, ...)
If you either need to use different keyword arguments for the source and target texts, you should do two calls like
this:
model_inputs = tokenizer(src_texts, ...)
labels = tokenizer(text_target=tgt_texts, ...)
model_inputs["labels"] = labels["input_ids"]
See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.
For a more complete example, see the implementation of `prepare_seq2seq_batch`.
  warnings.warn(formatted_warning, FutureWarning)
/local/home/CE/musaeed/transformers/src/transformers/tokenization_utils_base.py:3545: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/local/home/CE/musaeed/transformers/src/transformers/tokenization_utils_base.py:3671: FutureWarning:
`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular
`__call__` method to prepare your inputs and targets.
Here is a short example:
model_inputs = tokenizer(src_texts, text_target=tgt_texts, ...)
If you either need to use different keyword arguments for the source and target texts, you should do two calls like
this:
model_inputs = tokenizer(src_texts, ...)
labels = tokenizer(text_target=tgt_texts, ...)
model_inputs["labels"] = labels["input_ids"]
See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.
For a more complete example, see the implementation of `prepare_seq2seq_batch`.
  warnings.warn(formatted_warning, FutureWarning)
/local/home/CE/musaeed/transformers/src/transformers/tokenization_utils_base.py:3545: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/local/home/CE/musaeed/transformers/src/transformers/tokenization_utils_base.py:3671: FutureWarning:
`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular
`__call__` method to prepare your inputs and targets.
Here is a short example:
model_inputs = tokenizer(src_texts, text_target=tgt_texts, ...)
If you either need to use different keyword arguments for the source and target texts, you should do two calls like
this:
model_inputs = tokenizer(src_texts, ...)
labels = tokenizer(text_target=tgt_texts, ...)
model_inputs["labels"] = labels["input_ids"]
See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.
For a more complete example, see the implementation of `prepare_seq2seq_batch`.
  warnings.warn(formatted_warning, FutureWarning)
/local/home/CE/musaeed/transformers/src/transformers/tokenization_utils_base.py:3671: FutureWarning:
`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular
`__call__` method to prepare your inputs and targets.
Here is a short example:
model_inputs = tokenizer(src_texts, text_target=tgt_texts, ...)
If you either need to use different keyword arguments for the source and target texts, you should do two calls like
this:
model_inputs = tokenizer(src_texts, ...)
labels = tokenizer(text_target=tgt_texts, ...)
model_inputs["labels"] = labels["input_ids"]
See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.
For a more complete example, see the implementation of `prepare_seq2seq_batch`.
  warnings.warn(formatted_warning, FutureWarning)
/local/home/CE/musaeed/transformers/src/transformers/tokenization_utils_base.py:3545: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/local/home/CE/musaeed/transformers/src/transformers/tokenization_utils_base.py:3545: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/local/home/CE/musaeed/transformers/src/transformers/tokenization_utils_base.py:3671: FutureWarning:
`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular
`__call__` method to prepare your inputs and targets.
Here is a short example:
model_inputs = tokenizer(src_texts, text_target=tgt_texts, ...)
If you either need to use different keyword arguments for the source and target texts, you should do two calls like
this:
model_inputs = tokenizer(src_texts, ...)
labels = tokenizer(text_target=tgt_texts, ...)
model_inputs["labels"] = labels["input_ids"]
See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.
For a more complete example, see the implementation of `prepare_seq2seq_batch`.
  warnings.warn(formatted_warning, FutureWarning)
/local/home/CE/musaeed/transformers/src/transformers/tokenization_utils_base.py:3545: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/local/home/CE/musaeed/transformers/src/transformers/tokenization_utils_base.py:3671: FutureWarning:
`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular
`__call__` method to prepare your inputs and targets.
Here is a short example:
model_inputs = tokenizer(src_texts, text_target=tgt_texts, ...)
If you either need to use different keyword arguments for the source and target texts, you should do two calls like
this:
model_inputs = tokenizer(src_texts, ...)
labels = tokenizer(text_target=tgt_texts, ...)
model_inputs["labels"] = labels["input_ids"]
See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.
For a more complete example, see the implementation of `prepare_seq2seq_batch`.
  warnings.warn(formatted_warning, FutureWarning)
/local/home/CE/musaeed/transformers/src/transformers/tokenization_utils_base.py:3545: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/local/home/CE/musaeed/transformers/src/transformers/tokenization_utils_base.py:3671: FutureWarning:
`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular
`__call__` method to prepare your inputs and targets.
Here is a short example:
model_inputs = tokenizer(src_texts, text_target=tgt_texts, ...)
If you either need to use different keyword arguments for the source and target texts, you should do two calls like
this:
model_inputs = tokenizer(src_texts, ...)
labels = tokenizer(text_target=tgt_texts, ...)
model_inputs["labels"] = labels["input_ids"]
See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.
For a more complete example, see the implementation of `prepare_seq2seq_batch`.
  warnings.warn(formatted_warning, FutureWarning)
/local/home/CE/musaeed/transformers/src/transformers/tokenization_utils_base.py:3545: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(


Traceback (most recent call last):       | 3001/4206 [00:13<00:03, 386.41it/s]
  File "/home/CE/musaeed/Naija-Pidgin/mBART/translation/02_training_translation.py", line 54, in <module>
    model.train_model(train_df, eval_data=eval_df)
  File "/home/CE/musaeed/anaconda3/envs/t5/lib/python3.9/site-packages/simpletransformers/seq2seq/seq2seq_model.py", line 447, in train_model
    global_step, training_details = self.train(
  File "/home/CE/musaeed/anaconda3/envs/t5/lib/python3.9/site-packages/simpletransformers/seq2seq/seq2seq_model.py", line 830, in train
    results = self.eval_model(
  File "/home/CE/musaeed/anaconda3/envs/t5/lib/python3.9/site-packages/simpletransformers/seq2seq/seq2seq_model.py", line 1150, in eval_model
    result = self.evaluate(
  File "/home/CE/musaeed/anaconda3/envs/t5/lib/python3.9/site-packages/simpletransformers/seq2seq/seq2seq_model.py", line 1212, in evaluate
    outputs = model(**inputs)
  File "/home/CE/musaeed/anaconda3/envs/t5/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/local/home/CE/musaeed/transformers/src/transformers/models/mbart/modeling_mbart.py", line 1370, in forward
    lm_logits = self.lm_head(outputs[0]) + self.final_logits_bias
RuntimeError: CUDA out of memory. Tried to allocate 7.63 GiB (GPU 0; 31.75 GiB total capacity; 25.42 GiB already allocated; 4.27 GiB free; 26.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[31mâ•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ [39m[1mTraceback (most recent call last)[31m[22m â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
[31mâ”‚[39m /home/CE/musaeed/Naija-Pidgin/mBART/translation/[1m02_training_translation.py[22m [31mâ”‚
[31mâ”‚[39m :[94m54[39m in [92m<module>[39m                                                            [31mâ”‚
[31mâ”‚[39m                                                                            [31mâ”‚
[31mâ”‚[39m   51 â”‚   use_cuda=[94mTrue[39m                                                     [31mâ”‚
[31mâ”‚[39m   52 )                                                                     [31mâ”‚
[31mâ”‚[39m   53                                                                       [31mâ”‚
[31mâ”‚[39m [31mâ± [39m54 model.train_model(train_df, eval_data=eval_df)                        [31mâ”‚
[31mâ”‚[39m   55                                                                       [31mâ”‚
[31mâ”‚[39m   56 to_predict = [                                                        [31mâ”‚
[31mâ”‚[39m   57 â”‚   prefix + [33m": "[39m + [96mstr[39m(input_text)                                   [31mâ”‚
[31mâ”‚[39m                                                                            [31mâ”‚
[31mâ”‚[39m /home/CE/musaeed/anaconda3/envs/t5/lib/python3.9/site-packages/simpletrans [31mâ”‚
[31mâ”‚[39m formers/seq2seq/[1mseq2seq_model.py[22m:[94m447[39m in [92mtrain_model[39m                        [31mâ”‚
[31mâ”‚[39m                                                                            [31mâ”‚
[31mâ”‚[39m    444 â”‚   â”‚                                                               [31mâ”‚
[31mâ”‚[39m    445 â”‚   â”‚   os.makedirs(output_dir, exist_ok=[94mTrue[39m)                      [31mâ”‚
[31mâ”‚[39m    446 â”‚   â”‚                                                               [31mâ”‚
[31mâ”‚[39m [31mâ± [39m 447 â”‚   â”‚   global_step, training_details = [96mself[39m.train(                 [31mâ”‚
[31mâ”‚[39m    448 â”‚   â”‚   â”‚   train_dataset,                                          [31mâ”‚
[31mâ”‚[39m    449 â”‚   â”‚   â”‚   output_dir,                                             [31mâ”‚
[31mâ”‚[39m    450 â”‚   â”‚   â”‚   show_running_loss=show_running_loss,                    [31mâ”‚
[31mâ”‚[39m                                                                            [31mâ”‚
[31mâ”‚[39m /home/CE/musaeed/anaconda3/envs/t5/lib/python3.9/site-packages/simpletrans [31mâ”‚
[31mâ”‚[39m formers/seq2seq/[1mseq2seq_model.py[22m:[94m830[39m in [92mtrain[39m                              [31mâ”‚
[31mâ”‚[39m                                                                            [31mâ”‚
[31mâ”‚[39m    827 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   [95mand[39m global_step % args.evaluate_during_trai [31mâ”‚
[31mâ”‚[39m    828 â”‚   â”‚   â”‚   â”‚   â”‚   ):                                              [31mâ”‚
[31mâ”‚[39m    829 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   # Only evaluate when single GPU otherwise m [31mâ”‚
[31mâ”‚[39m [31mâ± [39m 830 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   results = [96mself[39m.eval_model(                  [31mâ”‚
[31mâ”‚[39m    831 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   eval_data,                              [31mâ”‚
[31mâ”‚[39m    832 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   verbose=verbose [95mand[39m args.evaluate_durin [31mâ”‚
[31mâ”‚[39m    833 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   silent=args.evaluate_during_training_si [31mâ”‚
[31mâ”‚[39m                                                                            [31mâ”‚
[31mâ”‚[39m /home/CE/musaeed/anaconda3/envs/t5/lib/python3.9/site-packages/simpletrans [31mâ”‚
[31mâ”‚[39m formers/seq2seq/[1mseq2seq_model.py[22m:[94m1150[39m in [92meval_model[39m                        [31mâ”‚
[31mâ”‚[39m                                                                            [31mâ”‚
[31mâ”‚[39m   1147 â”‚   â”‚   )                                                           [31mâ”‚
[31mâ”‚[39m   1148 â”‚   â”‚   os.makedirs(output_dir, exist_ok=[94mTrue[39m)                      [31mâ”‚
[31mâ”‚[39m   1149 â”‚   â”‚                                                               [31mâ”‚
[31mâ”‚[39m [31mâ± [39m1150 â”‚   â”‚   result = [96mself[39m.evaluate(                                     [31mâ”‚
[31mâ”‚[39m   1151 â”‚   â”‚   â”‚   eval_dataset, output_dir, verbose=verbose, silent=silen [31mâ”‚
[31mâ”‚[39m   1152 â”‚   â”‚   )                                                           [31mâ”‚
[31mâ”‚[39m   1153 â”‚   â”‚   [96mself[39m.results.update(result)                                 [31mâ”‚
[31mâ”‚[39m                                                                            [31mâ”‚
[31mâ”‚[39m /home/CE/musaeed/anaconda3/envs/t5/lib/python3.9/site-packages/simpletrans [31mâ”‚
[31mâ”‚[39m formers/seq2seq/[1mseq2seq_model.py[22m:[94m1212[39m in [92mevaluate[39m                          [31mâ”‚
[31mâ”‚[39m                                                                            [31mâ”‚
[31mâ”‚[39m   1209 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   outputs = model(**inputs)                   [31mâ”‚
[31mâ”‚[39m   1210 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   tmp_eval_loss = outputs[[94m0[39m]                  [31mâ”‚
[31mâ”‚[39m   1211 â”‚   â”‚   â”‚   â”‚   [94melse[39m:                                               [31mâ”‚
[31mâ”‚[39m [31mâ± [39m1212 â”‚   â”‚   â”‚   â”‚   â”‚   outputs = model(**inputs)                       [31mâ”‚
[31mâ”‚[39m   1213 â”‚   â”‚   â”‚   â”‚   â”‚   tmp_eval_loss = outputs[[94m0[39m]                      [31mâ”‚
[31mâ”‚[39m   1214 â”‚   â”‚   â”‚   â”‚   [94mif[39m [96mself[39m.args.n_gpu > [94m1[39m:                             [31mâ”‚
[31mâ”‚[39m   1215 â”‚   â”‚   â”‚   â”‚   â”‚   tmp_eval_loss = tmp_eval_loss.mean()            [31mâ”‚
[31mâ”‚[39m                                                                            [31mâ”‚
[31mâ”‚[39m /home/CE/musaeed/anaconda3/envs/t5/lib/python3.9/site-packages/torch/nn/mo [31mâ”‚
[31mâ”‚[39m dules/[1mmodule.py[22m:[94m1130[39m in [92m_call_impl[39m                                         [31mâ”‚
[31mâ”‚[39m                                                                            [31mâ”‚
[31mâ”‚[39m   1127 â”‚   â”‚   # this function, and just call forward.                     [31mâ”‚
[31mâ”‚[39m   1128 â”‚   â”‚   [94mif[39m [95mnot[39m ([96mself[39m._backward_hooks [95mor[39m [96mself[39m._forward_hooks [95mor[39m [96mself[39m [31mâ”‚
[31mâ”‚[39m   1129 â”‚   â”‚   â”‚   â”‚   [95mor[39m _global_forward_hooks [95mor[39m _global_forward_pre_hoo [31mâ”‚
[31mâ”‚[39m [31mâ± [39m1130 â”‚   â”‚   â”‚   [94mreturn[39m forward_call(*[96minput[39m, **kwargs)                   [31mâ”‚
[31mâ”‚[39m   1131 â”‚   â”‚   # Do not call functions when jit is used                    [31mâ”‚
[31mâ”‚[39m   1132 â”‚   â”‚   full_backward_hooks, non_full_backward_hooks = [], []       [31mâ”‚
[31mâ”‚[39m   1133 â”‚   â”‚   [94mif[39m [96mself[39m._backward_hooks [95mor[39m _global_backward_hooks:          [31mâ”‚
[31mâ”‚[39m                                                                            [31mâ”‚
[31mâ”‚[39m /local/home/CE/musaeed/transformers/src/transformers/models/mbart/[1mmodeling[22m [31mâ”‚
[31mâ”‚[39m [1m_mbart.py[22m:[94m1370[39m in [92mforward[39m                                                  [31mâ”‚
[31mâ”‚[39m                                                                            [31mâ”‚
[31mâ”‚[39m   1367 â”‚   â”‚   â”‚   output_hidden_states=output_hidden_states,              [31mâ”‚
[31mâ”‚[39m   1368 â”‚   â”‚   â”‚   return_dict=return_dict,                                [31mâ”‚
[31mâ”‚[39m   1369 â”‚   â”‚   )                                                           [31mâ”‚
[31mâ”‚[39m [31mâ± [39m1370 â”‚   â”‚   lm_logits = [96mself[39m.lm_head(outputs[[94m0[39m]) + [96mself[39m.final_logits_bi [31mâ”‚
[31mâ”‚[39m   1371 â”‚   â”‚                                                               [31mâ”‚
[31mâ”‚[39m   1372 â”‚   â”‚   masked_lm_loss = [94mNone[39m                                       [31mâ”‚
[31mâ”‚[39m   1373 â”‚   â”‚   [94mif[39m labels [95mis[39m [95mnot[39m [94mNone[39m:                                      [31mâ”‚
[31mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[1mRuntimeError: [22mCUDA out of memory. Tried to allocate [1m7.63[22m GiB [1m([22mGPU [1m0[22m; [1m31.75[22m GiB
total capacity; [1m25.42[22m GiB already allocated; [1m4.27[22m GiB free; [1m26.31[22m GiB reserved
in total by PyTorch[1m)[22m If reserved memory is >> allocated memory try setting
max_split_size_mb to avoid fragmentation.  See documentation for Memory
Management and PYTORCH_CUDA_ALLOC_CONF
2022-10-22 19:47:33,543 INFO    MainThread:3465 [wandb_setup.py:_flush():68] Configure stats pid to 3465
2022-10-22 19:47:33,544 INFO    MainThread:3465 [wandb_setup.py:_flush():68] Loading settings from /home/CE/musaeed/.config/wandb/settings
2022-10-22 19:47:33,544 INFO    MainThread:3465 [wandb_setup.py:_flush():68] Loading settings from /local/home/CE/musaeed/Naija-Pidgin/wandb/settings
2022-10-22 19:47:33,544 INFO    MainThread:3465 [wandb_setup.py:_flush():68] Loading settings from environment variables: {'_require_service': 'True'}
2022-10-22 19:47:33,544 WARNING MainThread:3465 [wandb_setup.py:_flush():68] Could not save program above cwd: /home/CE/musaeed/Naija-Pidgin/mBART/translation/02_training_translation.py
2022-10-22 19:47:33,544 INFO    MainThread:3465 [wandb_setup.py:_flush():68] Inferring run settings from compute environment: {'program_relpath': None, 'program': '/home/CE/musaeed/Naija-Pidgin/mBART/translation/02_training_translation.py'}
2022-10-22 19:47:33,544 INFO    MainThread:3465 [wandb_init.py:_log_setup():476] Logging user logs to /local/home/CE/musaeed/Naija-Pidgin/wandb/run-20221022_194733-3allo0a2/logs/debug.log
2022-10-22 19:47:33,544 INFO    MainThread:3465 [wandb_init.py:_log_setup():477] Logging internal logs to /local/home/CE/musaeed/Naija-Pidgin/wandb/run-20221022_194733-3allo0a2/logs/debug-internal.log
2022-10-22 19:47:33,545 INFO    MainThread:3465 [wandb_init.py:init():516] calling init triggers
2022-10-22 19:47:33,545 INFO    MainThread:3465 [wandb_init.py:init():519] wandb.init called with sweep_config: {}
config: {'adafactor_beta1': None, 'adafactor_clip_threshold': 1.0, 'adafactor_decay_rate': -0.8, 'adafactor_eps': (1e-30, 0.001), 'adafactor_relative_step': True, 'adafactor_scale_parameter': True, 'adafactor_warmup_init': True, 'adam_betas': (0.9, 0.999), 'adam_epsilon': 1e-08, 'best_model_dir': 'outputs/best_model', 'cache_dir': 'cache_dir/', 'config': {}, 'cosine_schedule_num_cycles': 0.5, 'custom_layer_parameters': [], 'custom_parameter_groups': [], 'dataloader_num_workers': 0, 'do_lower_case': False, 'dynamic_quantize': False, 'early_stopping_consider_epochs': False, 'early_stopping_delta': 0, 'early_stopping_metric': 'eval_loss', 'early_stopping_metric_minimize': True, 'early_stopping_patience': 3, 'encoding': None, 'eval_batch_size': 64, 'evaluate_during_training': True, 'evaluate_during_training_silent': True, 'evaluate_during_training_steps': 2500, 'evaluate_during_training_verbose': True, 'evaluate_each_epoch': True, 'fp16': False, 'gradient_accumulation_steps': 1, 'learning_rate': 5e-05, 'local_rank': -1, 'logging_steps': 50, 'loss_type': None, 'loss_args': {}, 'manual_seed': None, 'max_grad_norm': 1.0, 'max_seq_length': 128, 'model_name': 'facebook/mbart-large-50', 'model_type': 'mbart', 'multiprocessing_chunksize': -1, 'n_gpu': 1, 'no_cache': False, 'no_save': False, 'not_saved_args': [], 'num_train_epochs': 2, 'optimizer': 'AdamW', 'output_dir': '/home/CE/musaeed/mbart/output_dir', 'overwrite_output_dir': True, 'polynomial_decay_schedule_lr_end': 1e-07, 'polynomial_decay_schedule_power': 1.0, 'process_count': 22, 'quantized_model': False, 'reprocess_input_data': True, 'save_best_model': True, 'save_eval_checkpoints': False, 'save_model_every_epoch': True, 'save_optimizer_and_scheduler': True, 'save_steps': -1, 'scheduler': 'linear_schedule_with_warmup', 'silent': False, 'skip_special_tokens': True, 'tensorboard_dir': None, 'thread_count': None, 'tokenizer_name': None, 'tokenizer_type': None, 'train_batch_size': 8, 'train_custom_parameters_only': False, 'use_cached_eval_features': False, 'use_early_stopping': False, 'use_hf_datasets': False, 'use_multiprocessing': False, 'use_multiprocessing_for_evaluation': True, 'wandb_kwargs': {}, 'wandb_project': 'Multi-lingual Translation with mBART', 'warmup_ratio': 0.06, 'warmup_steps': 1499, 'weight_decay': 0.0, 'model_class': 'Seq2SeqModel', 'base_marian_model_name': 'facebook/mbart-large-50', 'dataset_class': None, 'dataset_cache_dir': None, 'do_sample': True, 'early_stopping': True, 'evaluate_generated_text': False, 'faiss_d': 768, 'faiss_m': 128, 'include_title_in_knowledge_dataset': True, 'length_penalty': 2.0, 'max_length': 128, 'max_steps': -1, 'num_beams': None, 'num_return_sequences': 3, 'rag_embed_batch_size': 16, 'repetition_penalty': 1.0, 'save_knowledge_dataset': True, 'save_knowledge_dataset_with_checkpoints': False, 'split_text_character': ' ', 'split_text_n': 100, 'src_lang': 'en_XX', 'tgt_lang': 'ro_RO', 'top_k': 50, 'top_p': 0.95, 'use_multiprocessed_decoding': False}
2022-10-22 19:47:33,545 INFO    MainThread:3465 [wandb_init.py:init():569] starting backend
2022-10-22 19:47:33,545 INFO    MainThread:3465 [wandb_init.py:init():573] setting up manager
2022-10-22 19:47:33,551 INFO    MainThread:3465 [backend.py:_multiprocessing_setup():102] multiprocessing start_methods=fork,spawn,forkserver, using: spawn
2022-10-22 19:47:33,553 INFO    MainThread:3465 [wandb_init.py:init():580] backend started and connected
2022-10-22 19:47:33,564 INFO    MainThread:3465 [wandb_init.py:init():658] updated telemetry
2022-10-22 19:47:33,682 INFO    MainThread:3465 [wandb_init.py:init():693] communicating run to backend with 60 second timeout
2022-10-22 19:47:34,153 INFO    MainThread:3465 [wandb_run.py:_on_init():2000] communicating current version
2022-10-22 19:47:34,225 INFO    MainThread:3465 [wandb_run.py:_on_init():2004] got version response 
2022-10-22 19:47:34,225 INFO    MainThread:3465 [wandb_init.py:init():728] starting run threads in backend
2022-10-22 19:47:36,940 INFO    MainThread:3465 [wandb_run.py:_console_start():1980] atexit reg
2022-10-22 19:47:36,940 INFO    MainThread:3465 [wandb_run.py:_redirect():1838] redirect: SettingsConsole.WRAP_RAW
2022-10-22 19:47:36,943 INFO    MainThread:3465 [wandb_run.py:_redirect():1903] Wrapping output streams.
2022-10-22 19:47:36,943 INFO    MainThread:3465 [wandb_run.py:_redirect():1925] Redirects installed.
2022-10-22 19:47:36,944 INFO    MainThread:3465 [wandb_init.py:init():765] run started, returning control to user process
2022-10-22 19:47:36,945 INFO    MainThread:3465 [wandb_watch.py:watch():51] Watching
2022-10-22 19:47:42,545 WARNING MsgRouterThr:3465 [router.py:message_loop():77] message_loop has been closed

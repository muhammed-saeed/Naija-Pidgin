Epochs 1/2. Running Loss:    8.9282:   0%|                           | 0/12489 [00:01<?, ?it/s]
Traceback (most recent call last):2:   0%|                           | 0/12489 [00:00<?, ?it/s]
  File "/home/CE/musaeed/Naija-Pidgin/mBART/translation/02_training_translation.py", line 55, in <module>
    model.train_model(train_df, eval_data=eval_df)
  File "/home/CE/musaeed/anaconda3/envs/t5/lib/python3.9/site-packages/simpletransformers/seq2seq/seq2seq_model.py", line 447, in train_model
    global_step, training_details = self.train(
  File "/home/CE/musaeed/anaconda3/envs/t5/lib/python3.9/site-packages/simpletransformers/seq2seq/seq2seq_model.py", line 790, in train
    optimizer.step()
  File "/home/CE/musaeed/anaconda3/envs/t5/lib/python3.9/site-packages/torch/optim/lr_scheduler.py", line 65, in wrapper
    return wrapped(*args, **kwargs)
  File "/home/CE/musaeed/anaconda3/envs/t5/lib/python3.9/site-packages/torch/optim/optimizer.py", line 113, in wrapper
    return func(*args, **kwargs)
  File "/home/CE/musaeed/anaconda3/envs/t5/lib/python3.9/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/home/CE/musaeed/anaconda3/envs/t5/lib/python3.9/site-packages/torch/optim/adamw.py", line 146, in step
    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)
RuntimeError: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 31.75 GiB total capacity; 9.50 GiB already allocated; 12.19 MiB free; 9.53 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[31mâ•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ [39m[1mTraceback (most recent call last)[31m[22m â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
[31mâ”‚[39m /home/CE/musaeed/Naija-Pidgin/mBART/translation/[1m02_training_translation.py[22m:[94m55[39m in [92m<module>[39m   [31mâ”‚
[31mâ”‚[39m                                                                                             [31mâ”‚
[31mâ”‚[39m   52 â”‚   cuda_devices=[[94m6[39m]                                                                   [31mâ”‚
[31mâ”‚[39m   53 )                                                                                      [31mâ”‚
[31mâ”‚[39m   54                                                                                        [31mâ”‚
[31mâ”‚[39m [31mâ± [39m55 model.train_model(train_df, eval_data=eval_df)                                         [31mâ”‚
[31mâ”‚[39m   56                                                                                        [31mâ”‚
[31mâ”‚[39m   57 to_predict = [                                                                         [31mâ”‚
[31mâ”‚[39m   58 â”‚   prefix + [33m": "[39m + [96mstr[39m(input_text)                                                    [31mâ”‚
[31mâ”‚[39m                                                                                             [31mâ”‚
[31mâ”‚[39m /home/CE/musaeed/anaconda3/envs/t5/lib/python3.9/site-packages/simpletransformers/seq2seq/[1ms[22m [31mâ”‚
[31mâ”‚[39m [1meq2seq_model.py[22m:[94m447[39m in [92mtrain_model[39m                                                          [31mâ”‚
[31mâ”‚[39m                                                                                             [31mâ”‚
[31mâ”‚[39m    444 â”‚   â”‚                                                                                [31mâ”‚
[31mâ”‚[39m    445 â”‚   â”‚   os.makedirs(output_dir, exist_ok=[94mTrue[39m)                                       [31mâ”‚
[31mâ”‚[39m    446 â”‚   â”‚                                                                                [31mâ”‚
[31mâ”‚[39m [31mâ± [39m 447 â”‚   â”‚   global_step, training_details = [96mself[39m.train(                                  [31mâ”‚
[31mâ”‚[39m    448 â”‚   â”‚   â”‚   train_dataset,                                                           [31mâ”‚
[31mâ”‚[39m    449 â”‚   â”‚   â”‚   output_dir,                                                              [31mâ”‚
[31mâ”‚[39m    450 â”‚   â”‚   â”‚   show_running_loss=show_running_loss,                                     [31mâ”‚
[31mâ”‚[39m                                                                                             [31mâ”‚
[31mâ”‚[39m /home/CE/musaeed/anaconda3/envs/t5/lib/python3.9/site-packages/simpletransformers/seq2seq/[1ms[22m [31mâ”‚
[31mâ”‚[39m [1meq2seq_model.py[22m:[94m790[39m in [92mtrain[39m                                                                [31mâ”‚
[31mâ”‚[39m                                                                                             [31mâ”‚
[31mâ”‚[39m    787 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   scaler.step(optimizer)                                       [31mâ”‚
[31mâ”‚[39m    788 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   scaler.update()                                              [31mâ”‚
[31mâ”‚[39m    789 â”‚   â”‚   â”‚   â”‚   â”‚   [94melse[39m:                                                            [31mâ”‚
[31mâ”‚[39m [31mâ± [39m 790 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   optimizer.step()                                             [31mâ”‚
[31mâ”‚[39m    791 â”‚   â”‚   â”‚   â”‚   â”‚   scheduler.step()  # Update learning rate schedule                [31mâ”‚
[31mâ”‚[39m    792 â”‚   â”‚   â”‚   â”‚   â”‚   model.zero_grad()                                                [31mâ”‚
[31mâ”‚[39m    793 â”‚   â”‚   â”‚   â”‚   â”‚   global_step += [94m1[39m                                                 [31mâ”‚
[31mâ”‚[39m                                                                                             [31mâ”‚
[31mâ”‚[39m /home/CE/musaeed/anaconda3/envs/t5/lib/python3.9/site-packages/torch/optim/[1mlr_scheduler.py[22m: [31mâ”‚
[31mâ”‚[39m [94m65[39m in [92mwrapper[39m                                                                               [31mâ”‚
[31mâ”‚[39m                                                                                             [31mâ”‚
[31mâ”‚[39m     62 â”‚   â”‚   â”‚   â”‚   instance = instance_ref()                                            [31mâ”‚
[31mâ”‚[39m     63 â”‚   â”‚   â”‚   â”‚   instance._step_count += [94m1[39m                                            [31mâ”‚
[31mâ”‚[39m     64 â”‚   â”‚   â”‚   â”‚   wrapped = func.[92m__get__[39m(instance, [96mcls[39m)                                [31mâ”‚
[31mâ”‚[39m [31mâ± [39m  65 â”‚   â”‚   â”‚   â”‚   [94mreturn[39m wrapped(*args, **kwargs)                                      [31mâ”‚
[31mâ”‚[39m     66 â”‚   â”‚   â”‚                                                                            [31mâ”‚
[31mâ”‚[39m     67 â”‚   â”‚   â”‚   # Note that the returned function here is no longer a bound method,      [31mâ”‚
[31mâ”‚[39m     68 â”‚   â”‚   â”‚   # so attributes like `__func__` and `__self__` no longer exist.          [31mâ”‚
[31mâ”‚[39m                                                                                             [31mâ”‚
[31mâ”‚[39m /home/CE/musaeed/anaconda3/envs/t5/lib/python3.9/site-packages/torch/optim/[1moptimizer.py[22m:[94m113[39m [31mâ”‚
[31mâ”‚[39m in [92mwrapper[39m                                                                                  [31mâ”‚
[31mâ”‚[39m                                                                                             [31mâ”‚
[31mâ”‚[39m   110 â”‚   â”‚   â”‚   â”‚   obj, *_ = args                                                        [31mâ”‚
[31mâ”‚[39m   111 â”‚   â”‚   â”‚   â”‚   profile_name = [33m"Optimizer.step#{}.step"[39m.format(obj.[91m__class__[39m.[91m__name__[39m [31mâ”‚
[31mâ”‚[39m   112 â”‚   â”‚   â”‚   â”‚   [94mwith[39m torch.autograd.profiler.record_function(profile_name):           [31mâ”‚
[31mâ”‚[39m [31mâ± [39m113 â”‚   â”‚   â”‚   â”‚   â”‚   [94mreturn[39m func(*args, **kwargs)                                      [31mâ”‚
[31mâ”‚[39m   114 â”‚   â”‚   â”‚   [94mreturn[39m wrapper                                                            [31mâ”‚
[31mâ”‚[39m   115 â”‚   â”‚                                                                                 [31mâ”‚
[31mâ”‚[39m   116 â”‚   â”‚   hooked = [96mgetattr[39m([96mself[39m.[91m__class__[39m.step, [33m"hooked"[39m, [94mNone[39m)                         [31mâ”‚
[31mâ”‚[39m                                                                                             [31mâ”‚
[31mâ”‚[39m /home/CE/musaeed/anaconda3/envs/t5/lib/python3.9/site-packages/torch/autograd/[1mgrad_mode.py[22m: [31mâ”‚
[31mâ”‚[39m [94m27[39m in [92mdecorate_context[39m                                                                      [31mâ”‚
[31mâ”‚[39m                                                                                             [31mâ”‚
[31mâ”‚[39m    24 â”‚   â”‚   [1m@functools[22m.wraps(func)                                                        [31mâ”‚
[31mâ”‚[39m    25 â”‚   â”‚   [94mdef[39m [92mdecorate_context[39m(*args, **kwargs):                                        [31mâ”‚
[31mâ”‚[39m    26 â”‚   â”‚   â”‚   [94mwith[39m [96mself[39m.clone():                                                        [31mâ”‚
[31mâ”‚[39m [31mâ± [39m 27 â”‚   â”‚   â”‚   â”‚   [94mreturn[39m func(*args, **kwargs)                                          [31mâ”‚
[31mâ”‚[39m    28 â”‚   â”‚   [94mreturn[39m cast(F, decorate_context)                                              [31mâ”‚
[31mâ”‚[39m    29 â”‚                                                                                     [31mâ”‚
[31mâ”‚[39m    30 â”‚   [94mdef[39m [92m_wrap_generator[39m([96mself[39m, func):                                                  [31mâ”‚
[31mâ”‚[39m                                                                                             [31mâ”‚
[31mâ”‚[39m /home/CE/musaeed/anaconda3/envs/t5/lib/python3.9/site-packages/torch/optim/[1madamw.py[22m:[94m146[39m in  [31mâ”‚
[31mâ”‚[39m [92mstep[39m                                                                                        [31mâ”‚
[31mâ”‚[39m                                                                                             [31mâ”‚
[31mâ”‚[39m   143 â”‚   â”‚   â”‚   â”‚   â”‚   state[[33m'step'[39m] = torch.zeros(([94m1[39m,), dtype=torch.float, device=p.dev [31mâ”‚
[31mâ”‚[39m   144 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   [94mif[39m [96mself[39m.defaults[[33m'capturable'[39m] [94melse[39m torch.tensor([94m0.[39m)          [31mâ”‚
[31mâ”‚[39m   145 â”‚   â”‚   â”‚   â”‚   â”‚   # Exponential moving average of gradient values                   [31mâ”‚
[31mâ”‚[39m [31mâ± [39m146 â”‚   â”‚   â”‚   â”‚   â”‚   state[[33m'exp_avg'[39m] = torch.zeros_like(p, memory_format=torch.preser [31mâ”‚
[31mâ”‚[39m   147 â”‚   â”‚   â”‚   â”‚   â”‚   # Exponential moving average of squared gradient values           [31mâ”‚
[31mâ”‚[39m   148 â”‚   â”‚   â”‚   â”‚   â”‚   state[[33m'exp_avg_sq'[39m] = torch.zeros_like(p, memory_format=torch.pre [31mâ”‚
[31mâ”‚[39m   149 â”‚   â”‚   â”‚   â”‚   â”‚   [94mif[39m amsgrad:                                                       [31mâ”‚
[31mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[1mRuntimeError: [22mCUDA out of memory. Tried to allocate [1m16.00[22m MiB [1m([22mGPU [1m0[22m; [1m31.75[22m GiB total capacity;
[1m9.50[22m GiB already allocated; [1m12.19[22m MiB free; [1m9.53[22m GiB reserved in total by PyTorch[1m)[22m If reserved
memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See
documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF